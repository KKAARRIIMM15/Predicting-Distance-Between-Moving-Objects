{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KKAARRIIMM15/Calculating-Distance-Between-Moving-Objects/blob/main/Object_Tracking_with_Weighted_Graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## the Author Email  **karimeldeeb2001@gmail.com**\n",
        "## the Author what's up number  **+201555604511**"
      ],
      "metadata": {
        "id": "2A6WRPGz16pD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This project shows how to use Pytorch to implement **object tracking** using YOLOv12 and visualizes movement\n",
        "\n",
        "### and calculate distances among all tracked objects using **weighted graph data structure.** The weighted graph dynamically connects detected objects between frames, assigning edge weights based on Euclidean distance (pixel movement)."
      ],
      "metadata": {
        "id": "DV6_dZCYlGk8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tuqee71clGFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create (You Only Look Once) model version 12 using PyTorch"
      ],
      "metadata": {
        "id": "CLyCvR8KoQl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.init import constant_, xavier_uniform_\n",
        "\n",
        "def autopad(k, p=None, d=1):\n",
        "    if d > 1:\n",
        "        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\n",
        "    if p is None:\n",
        "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
        "    return p\n",
        "\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    default_act = nn.SiLU()  # default activation\n",
        "\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(c2)\n",
        "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "    def forward_fuse(self, x):\n",
        "        return self.act(self.conv(x))\n",
        "\n",
        "\n",
        "class Conv2(Conv):\n",
        "    def __init__(self, c1, c2, k=3, s=1, p=None, g=1, d=1, act=True):\n",
        "        super().__init__(c1, c2, k, s, p, g=g, d=d, act=act)\n",
        "        self.cv2 = nn.Conv2d(c1, c2, 1, s, autopad(1, p, d), groups=g, dilation=d, bias=False)  # add 1x1 conv\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.conv(x) + self.cv2(x)))\n",
        "\n",
        "    def forward_fuse(self, x):\n",
        "        return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "    def fuse_convs(self):\n",
        "        w = torch.zeros_like(self.conv.weight.data)\n",
        "        i = [x // 2 for x in w.shape[2:]]\n",
        "        w[:, :, i[0] : i[0] + 1, i[1] : i[1] + 1] = self.cv2.weight.data.clone()\n",
        "        self.conv.weight.data += w\n",
        "        self.__delattr__(\"cv2\")\n",
        "        self.forward = self.forward_fuse\n",
        "\n",
        "\n",
        "class LightConv(nn.Module):\n",
        "    def __init__(self, c1, c2, k=1, act=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv(c1, c2, 1, act=False)\n",
        "        self.conv2 = DWConv(c2, c2, k, act=act)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv2(self.conv1(x))\n",
        "\n",
        "\n",
        "class DWConv(Conv):\n",
        "    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):\n",
        "        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)\n",
        "\n",
        "\n",
        "class DWConvTranspose2d(nn.ConvTranspose2d):\n",
        "    def __init__(self, c1, c2, k=1, s=1, p1=0, p2=0):\n",
        "        super().__init__(c1, c2, k, s, p1, p2, groups=math.gcd(c1, c2))\n",
        "\n",
        "\n",
        "class ConvTranspose(nn.Module):\n",
        "    default_act = nn.SiLU()  # default activation\n",
        "\n",
        "    def __init__(self, c1, c2, k=2, s=2, p=0, bn=True, act=True):\n",
        "        super().__init__()\n",
        "        self.conv_transpose = nn.ConvTranspose2d(c1, c2, k, s, p, bias=not bn)\n",
        "        self.bn = nn.BatchNorm2d(c2) if bn else nn.Identity()\n",
        "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.conv_transpose(x)))\n",
        "\n",
        "    def forward_fuse(self, x):\n",
        "        return self.act(self.conv_transpose(x))\n",
        "\n",
        "\n",
        "class Focus(nn.Module):\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):\n",
        "        super().__init__()\n",
        "        self.conv = Conv(c1 * 4, c2, k, s, p, g, act=act)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(torch.cat((x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]), 1))\n",
        "\n",
        "class GhostConv(nn.Module):\n",
        "    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):\n",
        "        super().__init__()\n",
        "        c_ = c2 // 2\n",
        "        self.cv1 = Conv(c1, c_, k, s, None, g, act=act)\n",
        "        self.cv2 = Conv(c_, c_, 5, 1, None, c_, act=act)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.cv1(x)\n",
        "        return torch.cat((y, self.cv2(y)), 1)\n",
        "\n",
        "\n",
        "class RepConv(nn.Module):\n",
        "    default_act = nn.SiLU()  # default activation\n",
        "\n",
        "    def __init__(self, c1, c2, k=3, s=1, p=1, g=1, d=1, act=True, bn=False, deploy=False):\n",
        "        super().__init__()\n",
        "        assert k == 3 and p == 1\n",
        "        self.g = g\n",
        "        self.c1 = c1\n",
        "        self.c2 = c2\n",
        "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(num_features=c1) if bn and c2 == c1 and s == 1 else None\n",
        "        self.conv1 = Conv(c1, c2, k, s, p=p, g=g, act=False)\n",
        "        self.conv2 = Conv(c1, c2, 1, s, p=(p - k // 2), g=g, act=False)\n",
        "\n",
        "    def forward_fuse(self, x):\n",
        "        return self.act(self.conv(x))\n",
        "\n",
        "    def forward(self, x):\n",
        "        id_out = 0 if self.bn is None else self.bn(x)\n",
        "        return self.act(self.conv1(x) + self.conv2(x) + id_out)\n",
        "\n",
        "    def get_equivalent_kernel_bias(self):\n",
        "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.conv1)\n",
        "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.conv2)\n",
        "        kernelid, biasid = self._fuse_bn_tensor(self.bn)\n",
        "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
        "\n",
        "    @staticmethod\n",
        "    def _pad_1x1_to_3x3_tensor(kernel1x1):\n",
        "        if kernel1x1 is None:\n",
        "            return 0\n",
        "        else:\n",
        "            return torch.nn.functional.pad(kernel1x1, [1, 1, 1, 1])\n",
        "\n",
        "    def _fuse_bn_tensor(self, branch):\n",
        "        if branch is None:\n",
        "            return 0, 0\n",
        "        if isinstance(branch, Conv):\n",
        "            kernel = branch.conv.weight\n",
        "            running_mean = branch.bn.running_mean\n",
        "            running_var = branch.bn.running_var\n",
        "            gamma = branch.bn.weight\n",
        "            beta = branch.bn.bias\n",
        "            eps = branch.bn.eps\n",
        "        elif isinstance(branch, nn.BatchNorm2d):\n",
        "            if not hasattr(self, \"id_tensor\"):\n",
        "                input_dim = self.c1 // self.g\n",
        "                kernel_value = np.zeros((self.c1, input_dim, 3, 3), dtype=np.float32)\n",
        "                for i in range(self.c1):\n",
        "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
        "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
        "            kernel = self.id_tensor\n",
        "            running_mean = branch.running_mean\n",
        "            running_var = branch.running_var\n",
        "            gamma = branch.weight\n",
        "            beta = branch.bias\n",
        "            eps = branch.eps\n",
        "        std = (running_var + eps).sqrt()\n",
        "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
        "        return kernel * t, beta - running_mean * gamma / std\n",
        "\n",
        "    def fuse_convs(self):\n",
        "        if hasattr(self, \"conv\"):\n",
        "            return\n",
        "        kernel, bias = self.get_equivalent_kernel_bias()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=self.conv1.conv.in_channels,\n",
        "            out_channels=self.conv1.conv.out_channels,\n",
        "            kernel_size=self.conv1.conv.kernel_size,\n",
        "            stride=self.conv1.conv.stride,\n",
        "            padding=self.conv1.conv.padding,\n",
        "            dilation=self.conv1.conv.dilation,\n",
        "            groups=self.conv1.conv.groups,\n",
        "            bias=True,\n",
        "        ).requires_grad_(False)\n",
        "        self.conv.weight.data = kernel\n",
        "        self.conv.bias.data = bias\n",
        "\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels: int) -> None:\n",
        "        super().__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Conv2d(channels, channels, 1, 1, 0, bias=True)\n",
        "        self.act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x * self.act(self.fc(self.pool(x)))\n",
        "\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super().__init__()\n",
        "        assert kernel_size in {3, 7}, \"kernel size must be 3 or 7\"\n",
        "        padding = 3 if kernel_size == 7 else 1\n",
        "        self.cv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.act(self.cv1(torch.cat([torch.mean(x, 1, keepdim=True), torch.max(x, 1, keepdim=True)[0]], 1)))\n",
        "\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, c1, kernel_size=7):\n",
        "        super().__init__()\n",
        "        self.channel_attention = ChannelAttention(c1)\n",
        "        self.spatial_attention = SpatialAttention(kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.spatial_attention(self.channel_attention(x))\n",
        "\n",
        "\n",
        "class Concat(nn.Module):\n",
        "    def __init__(self, dimension=1):\n",
        "        super().__init__()\n",
        "        self.d = dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat(x, self.d)\n",
        "\n",
        "\n",
        "class Index(nn.Module):\n",
        "    def __init__(self, c1, c2, index=0):\n",
        "        super().__init__()\n",
        "        self.index = index\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[self.index]"
      ],
      "metadata": {
        "id": "lLjaphuEjwPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Detect(nn.Module):\n",
        "    dynamic = False\n",
        "    export = False\n",
        "    format = None\n",
        "    end2end = False\n",
        "    max_det = 300\n",
        "    shape = None\n",
        "    anchors = torch.empty(0)\n",
        "    strides = torch.empty(0)\n",
        "    legacy = False\n",
        "\n",
        "    def __init__(self, nc=80, ch=()):\n",
        "        super().__init__()\n",
        "        self.nc = nc\n",
        "        self.nl = len(ch)\n",
        "        self.reg_max = 16\n",
        "        self.no = nc + self.reg_max * 4\n",
        "        self.stride = torch.zeros(self.nl)\n",
        "        c2, c3 = max((16, ch[0] // 4, self.reg_max * 4)), max(ch[0], min(self.nc, 100))\n",
        "        self.cv2 = nn.ModuleList(\n",
        "            nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch\n",
        "        )\n",
        "        self.cv3 = (\n",
        "            nn.ModuleList(nn.Sequential(Conv(x, c3, 3), Conv(c3, c3, 3), nn.Conv2d(c3, self.nc, 1)) for x in ch)\n",
        "            if self.legacy\n",
        "            else nn.ModuleList(\n",
        "                nn.Sequential(\n",
        "                    nn.Sequential(DWConv(x, x, 3), Conv(x, c3, 1)),\n",
        "                    nn.Sequential(DWConv(c3, c3, 3), Conv(c3, c3, 1)),\n",
        "                    nn.Conv2d(c3, self.nc, 1),\n",
        "                )\n",
        "                for x in ch\n",
        "            )\n",
        "        )\n",
        "        self.dfl = DFL(self.reg_max) if self.reg_max > 1 else nn.Identity()\n",
        "\n",
        "        if self.end2end:\n",
        "            self.one2one_cv2 = copy.deepcopy(self.cv2)\n",
        "            self.one2one_cv3 = copy.deepcopy(self.cv3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.end2end:\n",
        "            return self.forward_end2end(x)\n",
        "\n",
        "        for i in range(self.nl):\n",
        "            x[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)\n",
        "        if self.training:  # Training path\n",
        "            return x\n",
        "        y = self._inference(x)\n",
        "        return y if self.export else (y, x)\n",
        "\n",
        "    def forward_end2end(self, x):\n",
        "        x_detach = [xi.detach() for xi in x]\n",
        "        one2one = [\n",
        "            torch.cat((self.one2one_cv2[i](x_detach[i]), self.one2one_cv3[i](x_detach[i])), 1) for i in range(self.nl)\n",
        "        ]\n",
        "        for i in range(self.nl):\n",
        "            x[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)\n",
        "        if self.training:  # Training path\n",
        "            return {\"one2many\": x, \"one2one\": one2one}\n",
        "\n",
        "        y = self._inference(one2one)\n",
        "        y = self.postprocess(y.permute(0, 2, 1), self.max_det, self.nc)\n",
        "        return y if self.export else (y, {\"one2many\": x, \"one2one\": one2one})\n",
        "\n",
        "    def _inference(self, x):\n",
        "        shape = x[0].shape\n",
        "        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)\n",
        "        if self.format != \"imx\" and (self.dynamic or self.shape != shape):\n",
        "            self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\n",
        "            self.shape = shape\n",
        "\n",
        "        if self.export and self.format in {\"saved_model\", \"pb\", \"tflite\", \"edgetpu\", \"tfjs\"}:\n",
        "            box = x_cat[:, : self.reg_max * 4]\n",
        "            cls = x_cat[:, self.reg_max * 4 :]\n",
        "        else:\n",
        "            box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)\n",
        "\n",
        "        if self.export and self.format in {\"tflite\", \"edgetpu\"}:\n",
        "            grid_h = shape[2]\n",
        "            grid_w = shape[3]\n",
        "            grid_size = torch.tensor([grid_w, grid_h, grid_w, grid_h], device=box.device).reshape(1, 4, 1)\n",
        "            norm = self.strides / (self.stride[0] * grid_size)\n",
        "            dbox = self.decode_bboxes(self.dfl(box) * norm, self.anchors.unsqueeze(0) * norm[:, :2])\n",
        "        elif self.export and self.format == \"imx\":\n",
        "            dbox = self.decode_bboxes(\n",
        "                self.dfl(box) * self.strides, self.anchors.unsqueeze(0) * self.strides, xywh=False\n",
        "            )\n",
        "            return dbox.transpose(1, 2), cls.sigmoid().permute(0, 2, 1)\n",
        "        else:\n",
        "            dbox = self.decode_bboxes(self.dfl(box), self.anchors.unsqueeze(0)) * self.strides\n",
        "\n",
        "        return torch.cat((dbox, cls.sigmoid()), 1)\n",
        "\n",
        "    def bias_init(self):\n",
        "        m = self\n",
        "        for a, b, s in zip(m.cv2, m.cv3, m.stride):\n",
        "            a[-1].bias.data[:] = 1.0\n",
        "            b[-1].bias.data[: m.nc] = math.log(5 / m.nc / (640 / s) ** 2)\n",
        "        if self.end2end:\n",
        "            for a, b, s in zip(m.one2one_cv2, m.one2one_cv3, m.stride):\n",
        "                a[-1].bias.data[:] = 1.0  # box\n",
        "                b[-1].bias.data[: m.nc] = math.log(5 / m.nc / (640 / s) ** 2)\n",
        "\n",
        "    def decode_bboxes(self, bboxes, anchors, xywh=True):\n",
        "        return dist2bbox(bboxes, anchors, xywh=xywh and (not self.end2end), dim=1)\n",
        "\n",
        "    @staticmethod\n",
        "    def postprocess(preds: torch.Tensor, max_det: int, nc: int = 80):\n",
        "        batch_size, anchors, _ = preds.shape\n",
        "        boxes, scores = preds.split([4, nc], dim=-1)\n",
        "        index = scores.amax(dim=-1).topk(min(max_det, anchors))[1].unsqueeze(-1)\n",
        "        boxes = boxes.gather(dim=1, index=index.repeat(1, 1, 4))\n",
        "        scores = scores.gather(dim=1, index=index.repeat(1, 1, nc))\n",
        "        scores, index = scores.flatten(1).topk(min(max_det, anchors))\n",
        "        i = torch.arange(batch_size)[..., None]  # batch indices\n",
        "        return torch.cat([boxes[i, index // nc], scores[..., None], (index % nc)[..., None].float()], dim=-1)\n",
        "\n",
        "\n",
        "class Segment(Detect):\n",
        "    def __init__(self, nc=80, nm=32, npr=256, ch=()):\n",
        "        super().__init__(nc, ch)\n",
        "        self.nm = nm\n",
        "        self.npr = npr\n",
        "        self.proto = Proto(ch[0], self.npr, self.nm)\n",
        "\n",
        "        c4 = max(ch[0] // 4, self.nm)\n",
        "        self.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nm, 1)) for x in ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.proto(x[0])\n",
        "        bs = p.shape[0]\n",
        "\n",
        "        mc = torch.cat([self.cv4[i](x[i]).view(bs, self.nm, -1) for i in range(self.nl)], 2)\n",
        "        x = Detect.forward(self, x)\n",
        "        if self.training:\n",
        "            return x, mc, p\n",
        "        return (torch.cat([x, mc], 1), p) if self.export else (torch.cat([x[0], mc], 1), (x[1], mc, p))\n",
        "\n",
        "class Pose(Detect):\n",
        "    def __init__(self, nc=80, kpt_shape=(17, 3), ch=()):\n",
        "        super().__init__(nc, ch)\n",
        "        self.kpt_shape = kpt_shape\n",
        "        self.nk = kpt_shape[0] * kpt_shape[1]\n",
        "\n",
        "        c4 = max(ch[0] // 4, self.nk)\n",
        "        self.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nk, 1)) for x in ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs = x[0].shape[0]  # batch size\n",
        "        kpt = torch.cat([self.cv4[i](x[i]).view(bs, self.nk, -1) for i in range(self.nl)], -1)  # (bs, 17*3, h*w)\n",
        "        x = Detect.forward(self, x)\n",
        "        if self.training:\n",
        "            return x, kpt\n",
        "        pred_kpt = self.kpts_decode(bs, kpt)\n",
        "        return torch.cat([x, pred_kpt], 1) if self.export else (torch.cat([x[0], pred_kpt], 1), (x[1], kpt))\n",
        "\n",
        "    def kpts_decode(self, bs, kpts):\n",
        "        ndim = self.kpt_shape[1]\n",
        "        if self.export:\n",
        "            if self.format in {\n",
        "                \"tflite\",\n",
        "                \"edgetpu\",\n",
        "            }:\n",
        "                y = kpts.view(bs, *self.kpt_shape, -1)\n",
        "                grid_h, grid_w = self.shape[2], self.shape[3]\n",
        "                grid_size = torch.tensor([grid_w, grid_h], device=y.device).reshape(1, 2, 1)\n",
        "                norm = self.strides / (self.stride[0] * grid_size)\n",
        "                a = (y[:, :, :2] * 2.0 + (self.anchors - 0.5)) * norm\n",
        "            else:\n",
        "\n",
        "                y = kpts.view(bs, *self.kpt_shape, -1)\n",
        "                a = (y[:, :, :2] * 2.0 + (self.anchors - 0.5)) * self.strides\n",
        "            if ndim == 3:\n",
        "                a = torch.cat((a, y[:, :, 2:3].sigmoid()), 2)\n",
        "            return a.view(bs, self.nk, -1)\n",
        "        else:\n",
        "            y = kpts.clone()\n",
        "            if ndim == 3:\n",
        "                y[:, 2::3] = y[:, 2::3].sigmoid()\n",
        "            y[:, 0::ndim] = (y[:, 0::ndim] * 2.0 + (self.anchors[0] - 0.5)) * self.strides\n",
        "            y[:, 1::ndim] = (y[:, 1::ndim] * 2.0 + (self.anchors[1] - 0.5)) * self.strides\n",
        "            return y\n",
        "\n",
        "\n",
        "class Classify(nn.Module):\n",
        "    export = False  # export mode\n",
        "\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1):\n",
        "        super().__init__()\n",
        "        c_ = 1280  # efficientnet_b0 size\n",
        "        self.conv = Conv(c1, c_, k, s, p, g)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)  # to x(b,c_,1,1)\n",
        "        self.drop = nn.Dropout(p=0.0, inplace=True)\n",
        "        self.linear = nn.Linear(c_, c2)  # to x(b,c2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Performs a forward pass of the YOLO model on input image data.\"\"\"\n",
        "        if isinstance(x, list):\n",
        "            x = torch.cat(x, 1)\n",
        "        x = self.linear(self.drop(self.pool(self.conv(x)).flatten(1)))\n",
        "        if self.training:\n",
        "            return x\n",
        "        y = x.softmax(1)  # get final output\n",
        "        return y if self.export else (y, x)\n",
        "\n",
        "class RTDETRDecoder(nn.Module):\n",
        "    def _generate_anchors(self, shapes, grid_size=0.05, dtype=torch.float32, device=\"cpu\", eps=1e-2):\n",
        "        anchors = []\n",
        "        for i, (h, w) in enumerate(shapes):\n",
        "            sy = torch.arange(end=h, dtype=dtype, device=device)\n",
        "            sx = torch.arange(end=w, dtype=dtype, device=device)\n",
        "            grid_y, grid_x = torch.meshgrid(sy, sx, indexing=\"ij\") if TORCH_1_10 else torch.meshgrid(sy, sx)\n",
        "            grid_xy = torch.stack([grid_x, grid_y], -1)  # (h, w, 2)\n",
        "\n",
        "            valid_WH = torch.tensor([w, h], dtype=dtype, device=device)\n",
        "            grid_xy = (grid_xy.unsqueeze(0) + 0.5) / valid_WH  # (1, h, w, 2)\n",
        "            wh = torch.ones_like(grid_xy, dtype=dtype, device=device) * grid_size * (2.0**i)\n",
        "            anchors.append(torch.cat([grid_xy, wh], -1).view(-1, h * w, 4))  # (1, h*w, 4)\n",
        "\n",
        "        anchors = torch.cat(anchors, 1)  # (1, h*w*nl, 4)\n",
        "        valid_mask = ((anchors > eps) & (anchors < 1 - eps)).all(-1, keepdim=True)  # 1, h*w*nl, 1\n",
        "        anchors = torch.log(anchors / (1 - anchors))\n",
        "        anchors = anchors.masked_fill(~valid_mask, float(\"inf\"))\n",
        "        return anchors, valid_mask\n",
        "\n",
        "    def _get_encoder_input(self, x):\n",
        "        x = [self.input_proj[i](feat) for i, feat in enumerate(x)]\n",
        "        feats = []\n",
        "        shapes = []\n",
        "        for feat in x:\n",
        "            h, w = feat.shape[2:]\n",
        "            feats.append(feat.flatten(2).permute(0, 2, 1))\n",
        "            shapes.append([h, w])\n",
        "\n",
        "        feats = torch.cat(feats, 1)\n",
        "        return feats, shapes\n",
        "\n",
        "    def _get_decoder_input(self, feats, shapes, dn_embed=None, dn_bbox=None):\n",
        "        bs = feats.shape[0]\n",
        "        anchors, valid_mask = self._generate_anchors(shapes, dtype=feats.dtype, device=feats.device)\n",
        "        features = self.enc_output(valid_mask * feats)\n",
        "\n",
        "        enc_outputs_scores = self.enc_score_head(features)\n",
        "\n",
        "        topk_ind = torch.topk(enc_outputs_scores.max(-1).values, self.num_queries, dim=1).indices.view(-1)\n",
        "        batch_ind = torch.arange(end=bs, dtype=topk_ind.dtype).unsqueeze(-1).repeat(1, self.num_queries).view(-1)\n",
        "        top_k_features = features[batch_ind, topk_ind].view(bs, self.num_queries, -1)\n",
        "        top_k_anchors = anchors[:, topk_ind].view(bs, self.num_queries, -1)\n",
        "        refer_bbox = self.enc_bbox_head(top_k_features) + top_k_anchors\n",
        "\n",
        "        enc_bboxes = refer_bbox.sigmoid()\n",
        "        if dn_bbox is not None:\n",
        "            refer_bbox = torch.cat([dn_bbox, refer_bbox], 1)\n",
        "        enc_scores = enc_outputs_scores[batch_ind, topk_ind].view(bs, self.num_queries, -1)\n",
        "\n",
        "        embeddings = self.tgt_embed.weight.unsqueeze(0).repeat(bs, 1, 1) if self.learnt_init_query else top_k_features\n",
        "        if self.training:\n",
        "            refer_bbox = refer_bbox.detach()\n",
        "            if not self.learnt_init_query:\n",
        "                embeddings = embeddings.detach()\n",
        "        if dn_embed is not None:\n",
        "            embeddings = torch.cat([dn_embed, embeddings], 1)\n",
        "\n",
        "        return embeddings, refer_bbox, enc_bboxes, enc_scores\n"
      ],
      "metadata": {
        "id": "1zUuEkxMK26X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def fuse_conv(conv, norm):\n",
        "    fused_conv = torch.nn.Conv2d(conv.in_channels,\n",
        "                                 conv.out_channels,\n",
        "                                 kernel_size=conv.kernel_size,\n",
        "                                 stride=conv.stride,\n",
        "                                 padding=conv.padding,\n",
        "                                 groups=conv.groups,\n",
        "                                 bias=True).requires_grad_(False).to(conv.weight.device)\n",
        "\n",
        "    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
        "    w_norm = torch.diag(norm.weight.div(torch.sqrt(norm.eps + norm.running_var)))\n",
        "    fused_conv.weight.copy_(torch.mm(w_norm, w_conv).view(fused_conv.weight.size()))\n",
        "\n",
        "    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias\n",
        "    b_norm = norm.bias - norm.weight.mul(norm.running_mean).div(torch.sqrt(norm.running_var + norm.eps))\n",
        "    fused_conv.bias.copy_(torch.mm(w_norm, b_conv.reshape(-1, 1)).reshape(-1) + b_norm)\n",
        "\n",
        "    return fused_conv\n",
        "\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, activation, k=1, s=1, p=0, g=1):\n",
        "        super().__init__()\n",
        "        self.conv = torch.nn.Conv2d(in_ch, out_ch, k, s, p, groups=g, bias=False)\n",
        "        self.norm = torch.nn.BatchNorm2d(out_ch, eps=0.001, momentum=0.03)\n",
        "        self.relu = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.norm(self.conv(x)))\n",
        "\n",
        "    def fuse_forward(self, x):\n",
        "        return self.relu(self.conv(x))\n",
        "\n",
        "\n",
        "class Residual(torch.nn.Module):\n",
        "    def __init__(self, ch, e=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv(ch, int(ch * e), torch.nn.SiLU(), k=3, p=1)\n",
        "        self.conv2 = Conv(int(ch * e), ch, torch.nn.SiLU(), k=3, p=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv2(self.conv1(x))\n",
        "\n",
        "\n",
        "class CSPModule(torch.nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv(in_ch, out_ch // 2, torch.nn.SiLU())\n",
        "        self.conv2 = Conv(in_ch, out_ch // 2, torch.nn.SiLU())\n",
        "        self.conv3 = Conv(2 * (out_ch // 2), out_ch, torch.nn.SiLU())\n",
        "        self.res_m = torch.nn.Sequential(Residual(out_ch // 2, e=1.0),\n",
        "                                         Residual(out_ch // 2, e=1.0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.res_m(self.conv1(x))\n",
        "        return self.conv3(torch.cat((y, self.conv2(x)), dim=1))\n",
        "\n",
        "\n",
        "class CSP(torch.nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, n, csp, r):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv(in_ch, 2 * (out_ch // r), torch.nn.SiLU())\n",
        "        self.conv2 = Conv((2 + n) * (out_ch // r), out_ch, torch.nn.SiLU())\n",
        "\n",
        "        if not csp:\n",
        "            self.res_m = torch.nn.ModuleList(Residual(out_ch // r) for _ in range(n))\n",
        "        else:\n",
        "            self.res_m = torch.nn.ModuleList(CSPModule(out_ch // r, out_ch // r) for _ in range(n))\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = list(self.conv1(x).chunk(2, 1))\n",
        "        y.extend(m(y[-1]) for m in self.res_m)\n",
        "        return self.conv2(torch.cat(y, dim=1))\n",
        "\n",
        "\n",
        "class SPP(torch.nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, k=5):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv(in_ch, in_ch // 2, torch.nn.SiLU())\n",
        "        self.conv2 = Conv(in_ch * 2, out_ch, torch.nn.SiLU())\n",
        "        self.res_m = torch.nn.MaxPool2d(k, stride=1, padding=k // 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        y1 = self.res_m(x)\n",
        "        y2 = self.res_m(y1)\n",
        "        return self.conv2(torch.cat(tensors=[x, y1, y2, self.res_m(y2)], dim=1))\n",
        "\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, ch, num_head):\n",
        "        super().__init__()\n",
        "        self.num_head = num_head\n",
        "        self.dim_head = ch // num_head\n",
        "        self.dim_key = self.dim_head // 2\n",
        "        self.scale = self.dim_key ** -0.5\n",
        "\n",
        "        self.qkv = Conv(ch, ch + self.dim_key * num_head * 2, torch.nn.Identity())\n",
        "\n",
        "        self.conv1 = Conv(ch, ch, torch.nn.Identity(), k=3, p=1, g=ch)\n",
        "        self.conv2 = Conv(ch, ch, torch.nn.Identity())\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.view(b, self.num_head, self.dim_key * 2 + self.dim_head, h * w)\n",
        "\n",
        "        q, k, v = qkv.split([self.dim_key, self.dim_key, self.dim_head], dim=2)\n",
        "\n",
        "        attn = (q.transpose(-2, -1) @ k) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        x = (v @ attn.transpose(-2, -1)).view(b, c, h, w) + self.conv1(v.reshape(b, c, h, w))\n",
        "        return self.conv2(x)\n",
        "\n",
        "\n",
        "class PSABlock(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, ch, num_head):\n",
        "        super().__init__()\n",
        "        self.conv1 = Attention(ch, num_head)\n",
        "        self.conv2 = torch.nn.Sequential(Conv(ch, ch * 2, torch.nn.SiLU()),\n",
        "                                         Conv(ch * 2, ch, torch.nn.Identity()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.conv1(x)\n",
        "        return x + self.conv2(x)\n",
        "\n",
        "\n",
        "class PSA(torch.nn.Module):\n",
        "    def __init__(self, ch, n):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv(ch, 2 * (ch // 2), torch.nn.SiLU())\n",
        "        self.conv2 = Conv(2 * (ch // 2), ch, torch.nn.SiLU())\n",
        "        self.res_m = torch.nn.Sequential(*(PSABlock(ch // 2, ch // 128) for _ in range(n)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, y = self.conv1(x).chunk(2, 1)\n",
        "        return self.conv2(torch.cat(tensors=(x, self.res_m(y)), dim=1))\n",
        "\n",
        "\n",
        "class DarkNet(torch.nn.Module):\n",
        "    def __init__(self, width, depth, csp):\n",
        "        super().__init__()\n",
        "        self.p1 = []\n",
        "        self.p2 = []\n",
        "        self.p3 = []\n",
        "        self.p4 = []\n",
        "        self.p5 = []\n",
        "\n",
        "        # p1/2\n",
        "        self.p1.append(Conv(width[0], width[1], torch.nn.SiLU(), k=3, s=2, p=1))\n",
        "        # p2/4\n",
        "        self.p2.append(Conv(width[1], width[2], torch.nn.SiLU(), k=3, s=2, p=1))\n",
        "        self.p2.append(CSP(width[2], width[3], depth[0], csp[0], r=4))\n",
        "        # p3/8\n",
        "        self.p3.append(Conv(width[3], width[3], torch.nn.SiLU(), k=3, s=2, p=1))\n",
        "        self.p3.append(CSP(width[3], width[4], depth[1], csp[0], r=4))\n",
        "        # p4/16\n",
        "        self.p4.append(Conv(width[4], width[4], torch.nn.SiLU(), k=3, s=2, p=1))\n",
        "        self.p4.append(CSP(width[4], width[4], depth[2], csp[1], r=2))\n",
        "        # p5/32\n",
        "        self.p5.append(Conv(width[4], width[5], torch.nn.SiLU(), k=3, s=2, p=1))\n",
        "        self.p5.append(CSP(width[5], width[5], depth[3], csp[1], r=2))\n",
        "        self.p5.append(SPP(width[5], width[5]))\n",
        "        self.p5.append(PSA(width[5], depth[4]))\n",
        "\n",
        "        self.p1 = torch.nn.Sequential(*self.p1)\n",
        "        self.p2 = torch.nn.Sequential(*self.p2)\n",
        "        self.p3 = torch.nn.Sequential(*self.p3)\n",
        "        self.p4 = torch.nn.Sequential(*self.p4)\n",
        "        self.p5 = torch.nn.Sequential(*self.p5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        p1 = self.p1(x)\n",
        "        p2 = self.p2(p1)\n",
        "        p3 = self.p3(p2)\n",
        "        p4 = self.p4(p3)\n",
        "        p5 = self.p5(p4)\n",
        "        return p3, p4, p5\n",
        "\n",
        "\n",
        "class DarkFPN(torch.nn.Module):\n",
        "    def __init__(self, width, depth, csp):\n",
        "        super().__init__()\n",
        "        self.up = torch.nn.Upsample(scale_factor=2)\n",
        "        self.h1 = CSP(width[4] + width[5], width[4], depth[5], csp[0], r=2)\n",
        "        self.h2 = CSP(width[4] + width[4], width[3], depth[5], csp[0], r=2)\n",
        "        self.h3 = Conv(width[3], width[3], torch.nn.SiLU(), k=3, s=2, p=1)\n",
        "        self.h4 = CSP(width[3] + width[4], width[4], depth[5], csp[0], r=2)\n",
        "        self.h5 = Conv(width[4], width[4], torch.nn.SiLU(), k=3, s=2, p=1)\n",
        "        self.h6 = CSP(width[4] + width[5], width[5], depth[5], csp[1], r=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        p3, p4, p5 = x\n",
        "        p4 = self.h1(torch.cat(tensors=[self.up(p5), p4], dim=1))\n",
        "        p3 = self.h2(torch.cat(tensors=[self.up(p4), p3], dim=1))\n",
        "        p4 = self.h4(torch.cat(tensors=[self.h3(p3), p4], dim=1))\n",
        "        p5 = self.h6(torch.cat(tensors=[self.h5(p4), p5], dim=1))\n",
        "        return p3, p4, p5\n",
        "\n",
        "\n",
        "class DFL(torch.nn.Module):\n",
        "    def __init__(self, ch=16):\n",
        "        super().__init__()\n",
        "        self.ch = ch\n",
        "        self.conv = torch.nn.Conv2d(ch, out_channels=1, kernel_size=1, bias=False).requires_grad_(False)\n",
        "        x = torch.arange(ch, dtype=torch.float).view(1, ch, 1, 1)\n",
        "        self.conv.weight.data[:] = torch.nn.Parameter(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, a = x.shape\n",
        "        x = x.view(b, 4, self.ch, a).transpose(2, 1)\n",
        "        return self.conv(x.softmax(1)).view(b, 4, a)\n",
        "\n",
        "\n",
        "class Head(torch.nn.Module):\n",
        "    anchors = torch.empty(0)\n",
        "    strides = torch.empty(0)\n",
        "\n",
        "    def __init__(self, nc=80, filters=()):\n",
        "        super().__init__()\n",
        "        self.ch = 16  # DFL channels\n",
        "        self.nc = nc  # number of classes\n",
        "        self.nl = len(filters)  # number of detection layers\n",
        "        self.no = nc + self.ch * 4  # number of outputs per anchor\n",
        "        self.stride = torch.zeros(self.nl)  # strides computed during build\n",
        "\n",
        "        box = max(64, filters[0] // 4)\n",
        "        cls = max(80, filters[0], self.nc)\n",
        "\n",
        "        self.dfl = DFL(self.ch)\n",
        "        self.box = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, box,torch.nn.SiLU(), k=3, p=1),\n",
        "                                                           Conv(box, box,torch.nn.SiLU(), k=3, p=1),\n",
        "                                                           torch.nn.Conv2d(box, out_channels=4 * self.ch,\n",
        "                                                                           kernel_size=1)) for x in filters)\n",
        "        self.cls = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, x, torch.nn.SiLU(), k=3, p=1, g=x),\n",
        "                                                           Conv(x, cls, torch.nn.SiLU()),\n",
        "                                                           Conv(cls, cls, torch.nn.SiLU(), k=3, p=1, g=cls),\n",
        "                                                           Conv(cls, cls, torch.nn.SiLU()),\n",
        "                                                           torch.nn.Conv2d(cls, out_channels=self.nc,\n",
        "                                                                           kernel_size=1)) for x in filters)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, (box, cls) in enumerate(zip(self.box, self.cls)):\n",
        "            x[i] = torch.cat(tensors=(box(x[i]), cls(x[i])), dim=1)\n",
        "        if self.training:\n",
        "            return x\n",
        "\n",
        "        self.anchors, self.strides = (i.transpose(0, 1) for i in make_anchors(x, self.stride))\n",
        "        x = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], dim=2)\n",
        "        box, cls = x.split(split_size=(4 * self.ch, self.nc), dim=1)\n",
        "\n",
        "        a, b = self.dfl(box).chunk(2, 1)\n",
        "        a = self.anchors.unsqueeze(0) - a\n",
        "        b = self.anchors.unsqueeze(0) + b\n",
        "        box = torch.cat(tensors=((a + b) / 2, b - a), dim=1)\n",
        "\n",
        "        return torch.cat(tensors=(box * self.strides, cls.sigmoid()), dim=1)\n",
        "\n",
        "    def initialize_biases(self):\n",
        "        for box, cls, s in zip(self.box, self.cls, self.stride):\n",
        "            # box\n",
        "            box[-1].bias.data[:] = 1.0\n",
        "            # cls (.01 objects, 80 classes, 640 image)\n",
        "            cls[-1].bias.data[:self.nc] = math.log(5 / self.nc / (640 / s) ** 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class YOLOv12(torch.nn.Module):\n",
        "    def __init__(self, width, depth, csp, num_classes):\n",
        "        super().__init__()\n",
        "        self.net = DarkNet(width, depth, csp)\n",
        "        self.fpn = DarkFPN(width, depth, csp)\n",
        "\n",
        "        img_dummy = torch.zeros(1, width[0], 256, 256)\n",
        "        self.head = Head(num_classes, (width[3], width[4], width[5]))\n",
        "        self.head.stride = torch.tensor([256 / x.shape[-2] for x in self.forward(img_dummy)])\n",
        "        self.stride = self.head.stride\n",
        "        self.head.initialize_biases()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        x = self.fpn(x)\n",
        "        return self.head(list(x))\n",
        "\n",
        "    def fuse(self):\n",
        "        for m in self.modules():\n",
        "            if type(m) is Conv and hasattr(m, 'norm'):\n",
        "                m.conv = fuse_conv(m.conv, m.norm)\n",
        "                m.forward = m.fuse_forward\n",
        "                delattr(m, 'norm')\n",
        "        return self\n"
      ],
      "metadata": {
        "id": "k2vkVnNnK28_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3G-eLpiwK2_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the created model"
      ],
      "metadata": {
        "id": "e0RSZRV0pMBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture(\"D:\\\\dataset\\\\Vehicle Dataset Sample 3.mp4\")\n",
        "ret, frame = cap.read()\n",
        "H, W, _ = frame.shape\n",
        "out = cv2.VideoWriter(\"D:\\\\dataset\\\\gj.mp4\", cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "                      int(cap.get(cv2.CAP_PROP_FPS)), (W, H))\n",
        "\n",
        "\n",
        "model = YOLOv12(\"D:\\\\dataset\\\\yolo12n.pt\")\n",
        "model.to(\"cuda\")\n",
        "threshold = 0.45\n",
        "\n",
        "# Graph for tracking object paths\n",
        "weighted_Graph = nx.Graph()\n",
        "pts = deque(maxlen=100)  # Store last 100 detected positions\n",
        "\n",
        "print(\"Model loaded\")\n",
        "while ret:\n",
        "    # Perform object detection\n",
        "    results = model(frame, imgsz=960, conf=0.45)[0]\n",
        "\n",
        "    for result in results.boxes.data.tolist():\n",
        "        x1, y1, x2, y2, score, class_id = result\n",
        "        if score > threshold:\n",
        "            # Calculate center point\n",
        "            center_x = int((x1 + x2) / 2)\n",
        "            center_y = int((y1 + y2) / 2)\n",
        "            center = (center_x, center_y)\n",
        "            pts.appendleft(center)\n",
        "\n",
        "            # Add node to the graph\n",
        "            weighted_Graph.add_node(len(pts), pos=center)\n",
        "\n",
        "            # Draw bounding box\n",
        "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "            cv2.circle(frame, center, 5, (0, 0, 255), -1)\n",
        "\n",
        "    # Draw graph edges and annotate with weights\n",
        "    for i in range(1, len(pts)):\n",
        "        if pts[i - 1] is None or pts[i] is None:\n",
        "            continue\n",
        "\n",
        "        # Compute Euclidean distance (weight of edge)\n",
        "        dist = int(np.linalg.norm(np.array(pts[i]) - np.array(pts[i - 1])))\n",
        "\n",
        "        # Add weighted edge to the graph\n",
        "        weighted_Graph.add_edge(i - 1, i, weight=dist)\n",
        "\n",
        "        # Draw the tracking line\n",
        "        cv2.line(frame, pts[i - 1], pts[i], (0, 0, 255), 1)\n",
        "\n",
        "        # Compute midpoint to place text\n",
        "        mid_x = (pts[i - 1][0] + pts[i][0]) // 2\n",
        "        mid_y = (pts[i - 1][1] + pts[i][1]) // 2\n",
        "\n",
        "        # Draw the weight (pixel length) on the frame\n",
        "        cv2.putText(frame, f\"{dist}px\", (mid_x, mid_y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
        "\n",
        "    # Write frame to output video\n",
        "    out.write(frame)\n",
        "\n",
        "    # Display frame\n",
        "    cv2.imshow('Object Tracking with Graph', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "    # Read next frame\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "U6LYfsFQK3Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9vZOmbU1K3EJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}